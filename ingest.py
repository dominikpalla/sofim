import os
import json
import requests
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import io
from pypdf import PdfReader
import docx  # Ponech√°v√°me pro p≈ô√≠padn√Ω budouc√≠ lok√°ln√≠ DOCX import

from config import OPENAI_API_KEY, EMBEDDING_MODEL, OPENAI_EMBEDDING_URL
from database import (
    prepare_next_table_for_update,
    insert_into_next_table,
    swap_tables_atomic,
    get_db_connection,
    set_sync_status,
    update_sync_progress,
    log_sync_error
)


# --- 1. Pomocn√© funkce pro CRAWLER ---

def get_urls_from_db():
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT url FROM crawler_urls")
        urls = [row[0] for row in cursor.fetchall()]
    except Exception as e:
        print(f"‚ö†Ô∏è Tabulka crawler_urls asi neexistuje nebo je pr√°zdn√°: {e}")
        urls = []
    finally:
        conn.close()
    return urls


def scrape_uhk_page(url):
    print(f"üï∏Ô∏è Crawluji: {url}")
    try:
        headers = {"User-Agent": "SofimBot/1.0 (UHK Internal)"}
        response = requests.get(url, headers=headers, timeout=15)

        if response.status_code != 200:
            print(f"   ‚ùå Chyba HTTP {response.status_code}")
            return None, []

        soup = BeautifulSoup(response.content, 'html.parser')

        pdf_urls = []
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            # Chyt√°me i podez≈ôel√© odkazy na detaily dokument≈Ø
            if '/file/' in href or '/download/' in href or href.lower().endswith('.pdf'):
                full_pdf_url = urljoin(url, href)
                if full_pdf_url not in pdf_urls:
                    pdf_urls.append(full_pdf_url)

        for element in soup(["script", "style", "noscript", "svg", "video", "iframe"]):
            element.decompose()

        html_for_ai = str(soup.body) if soup.body else str(soup)

        print("   ü§ñ Deleguji extrakci textu z HTML na umƒõlou inteligenci...")
        llm_headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}

        prompt = f"""
        Jsi expertn√≠ extraktor dat. Tv√Ωm √∫kolem je z n√°sleduj√≠c√≠ho zdrojov√©ho k√≥du webov√© str√°nky vyt√°hnout POUZE hlavn√≠ informaƒçn√≠ obsah.
        Pravidla:
        1. Ignoruj ve≈°ker√© navigaƒçn√≠ prvky (hlavn√≠ menu), patiƒçky, hlaviƒçky univerzity, cookie li≈°ty a podobn√Ω balast.
        2. Ignoruj texty tlaƒç√≠tek nesouvisej√≠c√≠ s obsahem (nap≈ô. "Sd√≠let na Facebooku", "Zpƒõt na √∫vod", "Vyhledat").
        3. Vra≈• absolutnƒõ ƒçist√Ω text, kter√Ω nese informaƒçn√≠ hodnotu.
        4. Neodpov√≠dej ≈æ√°dn√Ωmi √∫vodn√≠mi fr√°zemi (jako "Zde je text:"), prostƒõ rovnou vypi≈° extrahovan√Ω obsah.

        Obsah webu:
        {html_for_ai[:60000]}
        """

        data = {
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.0
        }

        llm_response = requests.post("https://api.openai.com/v1/chat/completions", headers=llm_headers, json=data,
                                     timeout=180)

        if llm_response.status_code == 200:
            clean_text = llm_response.json()["choices"][0]["message"]["content"].strip()

            if len(clean_text) < 20:
                print("   ‚ö†Ô∏è AI z t√©to str√°nky nedostala ≈æ√°dn√Ω smyslupln√Ω text.")
                return None, pdf_urls

            return clean_text, pdf_urls
        else:
            raise Exception(f"Chyba OpenAI p≈ôi extrakci HTML (HTTP {llm_response.status_code}): {llm_response.text}")

    except requests.exceptions.Timeout:
        raise Exception(f"Timeout: OpenAI API neodpovƒõdƒõlo p≈ôi extrakci HTML pro {url} vƒças.")
    except Exception as e:
        raise Exception(f"Chyba zpracov√°n√≠ {url}: {str(e)}")


def process_pdf_from_url(pdf_url, depth=0):
    """
    St√°hne PDF. Pokud naraz√≠ na HTML detail dokumentu, zkus√≠ v nƒõm naj√≠t skuteƒçn√© PDF.
    MAX hloubka zano≈ôen√≠ (depth) = 1, aby se nezacyklil.
    """
    # Pokud se zano≈ôujeme u≈æ podruh√© do HTML, radƒõji to ukonƒç√≠me
    if depth > 1:
        return None

    print(f"   üìÑ Zkoum√°m odkaz: {pdf_url}")
    try:
        headers = {"User-Agent": "SofimBot/1.0 (UHK Internal)"}
        response = requests.get(pdf_url, headers=headers, timeout=30)

        if response.status_code != 200:
            print(f"   ‚ùå Nelze st√°hnout (HTTP {response.status_code})")
            return None

        content_type = response.headers.get('Content-Type', '').lower()

        # SC√âN√Å≈ò A: M√°me p≈ô√≠mo ƒçist√© PDF
        if 'application/pdf' in content_type:
            print("   üîç Analyzuji PDF vrstvy...")
            fh = io.BytesIO(response.content)
            reader = PdfReader(fh)
            text = ""
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"

            if len(text.strip()) < 10:
                print(f"   ‚ö†Ô∏è PDF {pdf_url} je pravdƒõpodobnƒõ sken bez textov√© vrstvy.")
                return None

            print(f"   ‚úÖ PDF √∫spƒõ≈°nƒõ naƒçteno ({len(text)} znak≈Ø).")
            return text

        # SC√âN√Å≈ò B: Odkaz vede na podstr√°nku detailu dokumentu
        elif 'text/html' in content_type:
            if depth == 0:
                print(f"   üîÄ Odkaz vede na podstr√°nku, hled√°m skuteƒçn√© PDF uvnit≈ô...")
                soup = BeautifulSoup(response.content, 'html.parser')

                # Hled√°me skuteƒçn√Ω odkaz na soubor
                for a_tag in soup.find_all('a', href=True):
                    href = a_tag['href']
                    if '/file/' in href or '/download/' in href or 'stahnout' in href.lower() or href.lower().endswith(
                            '.pdf'):
                        real_pdf_url = urljoin(pdf_url, href)
                        # Pokud jsme na≈°li nov√Ω odkaz, zavol√°me stejnou funkci znovu (ale nastav√≠me hloubku)
                        if real_pdf_url != pdf_url:
                            return process_pdf_from_url(real_pdf_url, depth=depth + 1)

                print("   ‚ö†Ô∏è Na podstr√°nce se nepoda≈ôilo naj√≠t ≈æ√°dn√© dal≈°√≠ PDF.")
                return None
            else:
                return None

        # SC√âN√Å≈ò C: Je to ZIP, DOCX, obr√°zek atd.
        else:
            print(f"   ‚ö†Ô∏è Ignoruji: Soubor nen√≠ PDF (Typ: {content_type}).")
            return None

    except Exception as e:
        print(f"   ‚ùå Chyba ƒçten√≠ souboru {pdf_url}: {str(e)}")
        return None


# --- 2. Pomocn√© funkce pro CSV (Hybridn√≠ model) ---

def read_csv_smart(fh):
    encodings = ['utf-8', 'cp1250', 'latin1']
    for encoding in encodings:
        fh.seek(0)
        try:
            df = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, on_bad_lines='skip')
            keywords = ['zkratka', 'zkr_predm', 'nazev_cz', 'kredity', 'anotace_cz']
            col_str = str(list(df.columns)).lower()

            if not any(k in col_str for k in keywords):
                fh.seek(0)
                df_raw = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, header=None, on_bad_lines='skip',
                                     nrows=15)
                header_index = -1
                for i in range(len(df_raw)):
                    row_str = str(df_raw.iloc[i].values).lower()
                    if any(k in row_str for k in keywords):
                        header_index = i
                        break
                if header_index != -1:
                    fh.seek(0)
                    df = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, header=header_index,
                                     on_bad_lines='skip')

            df = df.dropna(how='all')
            df = df.fillna("")
            df.columns = [str(c).strip() for c in df.columns]
            return df
        except Exception:
            continue
    return None


# --- 3. Chunking funkce ---

def semantic_chunking(text, filename):
    if not text or len(text.strip()) < 10:
        return []

    print(f"üß† S√©mantick√© ≈ôez√°n√≠ obsahu: {filename}...")
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}

    chunk_size = 12000
    text_blocks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    all_extracted_chunks = []

    for idx, block in enumerate(text_blocks):
        if len(text_blocks) > 1:
            print(f"   ‚è≥ Zpracov√°v√°m ƒç√°st {idx + 1}/{len(text_blocks)}...")

        prompt = f"""
        Jsi expertn√≠ analytik. Rozdƒõl text na logick√© celky (chunky).
        Zdroj: {filename} (ƒå√°st {idx + 1} z {len(text_blocks)})
        Pravidla:
        1. V√Ωstup MUS√ç b√Ωt validn√≠ JSON.
        2. Form√°t: {{"chunks": [ {{"title": "...", "content": "..."}} ]}}
        Text k anal√Ωze:
        {block}
        """

        data = {
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": prompt}],
            "response_format": {"type": "json_object"},
            "temperature": 0.0
        }

        try:
            response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data,
                                     timeout=180)

            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                json_content = json.loads(content)

                if "chunks" in json_content:
                    all_extracted_chunks.extend(json_content["chunks"])
                elif "items" in json_content:
                    all_extracted_chunks.extend(json_content["items"])
            else:
                print(f"   ‚ö†Ô∏è API Error u ƒç√°sti {idx + 1} (HTTP {response.status_code}): {response.text}")

        except Exception as e:
            print(f"   ‚ö†Ô∏è Chyba AI chunkingu u ƒç√°sti {idx + 1}: {str(e)}")

    if not all_extracted_chunks:
        print("   ‚ö†Ô∏è S√©mantick√Ω chunking selhal nebo nevr√°til nic.")
        return []

    return all_extracted_chunks


def csv_row_chunking(df, filename):
    print(f"üìä Zpracov√°v√°m tabulku p≈ôedmƒõt≈Ø: {filename} ({len(df)} ≈ô√°dk≈Ø)...")
    chunks = []

    for index, row in df.iterrows():
        row_dict = row.to_dict()
        nazev = row_dict.get('NAZEV_CZ', row_dict.get('NAZEV_AN', 'Nezn√°m√Ω p≈ôedmƒõt'))
        kod = row_dict.get('ZKR_PREDM', '')

        if not kod:
            for k, v in row_dict.items():
                if 'zkr' in str(k).lower() and not kod: kod = str(v)

        if nazev == 'Nezn√°m√Ω p≈ôedmƒõt' and not kod:
            continue

        title = f"P≈ôedmƒõt: {nazev} ({kod})".strip()
        content_lines = [f"--- Detail p≈ôedmƒõtu: {title} ---"]

        priority_fields = {
            'NAZEV_AN': 'Anglick√Ω n√°zev', 'GARANTI': 'Garanti', 'VYUCUJICI': 'Vyuƒçuj√≠c√≠',
            'KREDITY': 'Kredity', 'ROK_VARIANTY': 'Rok varianty', 'ANOTACE_CZ': 'Anotace',
            'CIL_CZ': 'C√≠le p≈ôedmƒõtu', 'OSNOVA_CZ': 'Osnova', 'LITERATURA': 'Literatura',
            'POZADAVKY_CZ': 'Po≈æadavky', 'METODY_VYUKY_CZ': 'Metody', 'URL': 'Odkaz'
        }

        for key, label in priority_fields.items():
            if key in row_dict:
                val = str(row_dict[key]).strip()
                if val and val.lower() != 'nan':
                    content_lines.append(f"{label}: {val}")

        chunks.append({"title": title, "content": "\n".join(content_lines)})

    return chunks


# --- 4. Embedding ---

def get_embedding(text):
    if not text or not text.strip():
        return None

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    data = {"input": text, "model": EMBEDDING_MODEL}

    try:
        response = requests.post(OPENAI_EMBEDDING_URL, headers=headers, json=data, timeout=60)
        if response.status_code == 200:
            return np.array(response.json()["data"][0]["embedding"])
        else:
            print(f"   ‚ö†Ô∏è Chyba OpenAI Embeddings (HTTP {response.status_code})")
            return None
    except Exception as e:
        print(f"   ‚ö†Ô∏è Chyba p≈ôi tvorbƒõ embeddingu: {str(e)}")
        return None


# --- 5. HLAVN√ç LOGIKA INDEXACE ---

def run_ingest(mode="all"):
    print(f"üöÄ Startuji indexaci na pozad√≠ (Re≈æim: {mode})...")

    if mode in ["all", "web"]: set_sync_status("WEB", "running")
    if mode in ["all", "csv"]: set_sync_status("CSV", "running")

    try:
        prepare_next_table_for_update(mode)
        success_count = 0

        # --- F√ÅZE A: CRAWLER (Web UHK) ---
        if mode in ["all", "web"]:
            urls = get_urls_from_db()
            total_urls = len(urls)
            set_sync_status("WEB", "running", total=total_urls)

            if urls:
                print(f"üåç Nalezeno {total_urls} URL adres k indexaci.")
                for idx, url in enumerate(urls, 1):
                    try:
                        web_text, pdf_links = scrape_uhk_page(url)

                        if web_text:
                            chunks = semantic_chunking(web_text, f"Web: {url}")
                            for chunk in chunks:
                                title = chunk.get("title", "Webov√° str√°nka").strip()
                                content = chunk.get("content", "").strip()

                                if not content:
                                    continue

                                emb = get_embedding(f"URL: {url}\n{content}")
                                if emb is not None:
                                    insert_into_next_table(title, content, emb, url)
                                    print(f"   üíæ Web ulo≈æen: {title[:40]}...")
                                    success_count += 1

                        if pdf_links:
                            print(f"   üìé Nalezeno {len(pdf_links)} soubor≈Ø na odkazu {url}.")
                            for pdf_url in pdf_links:
                                pdf_text = process_pdf_from_url(pdf_url)
                                if pdf_text:
                                    chunks = semantic_chunking(pdf_text, f"PDF: {pdf_url.split('/')[-1]}")
                                    for chunk in chunks:
                                        title = chunk.get("title", "PDF Dokument").strip()
                                        content = chunk.get("content", "").strip()

                                        if not content:
                                            continue

                                        emb = get_embedding(f"Zdroj PDF: {pdf_url}\n{content}")
                                        if emb is not None:
                                            insert_into_next_table(title, content, emb, pdf_url)
                                            success_count += 1

                    except Exception as e:
                        log_sync_error("WEB", f"Chyba na {url}: {str(e)}")
                        print(f"   ‚ùå Chyba zpracov√°n√≠ webu {url}: {e}")

                    update_sync_progress("WEB", idx)
            else:
                print("‚ö†Ô∏è ≈Ω√°dn√° URL v datab√°zi. P≈ôidej je p≈ôes /admin.")

        # --- F√ÅZE B: LOK√ÅLN√ç CSV (Studijn√≠ pl√°ny) ---
        if mode in ["all", "csv"]:
            csv_path = "data/predmety.csv"
            if os.path.exists(csv_path):
                print(f"üìä Naƒç√≠t√°m lok√°ln√≠ CSV: {csv_path}")
                try:
                    with open(csv_path, "rb") as f:
                        df = read_csv_smart(f)

                    if df is not None:
                        csv_chunks = csv_row_chunking(df, "Lok√°ln√≠ Datab√°ze P≈ôedmƒõt≈Ø")
                        total_rows = len(csv_chunks)
                        set_sync_status("CSV", "running", total=total_rows)

                        for idx, chunk in enumerate(csv_chunks, 1):
                            try:
                                emb = get_embedding(chunk["content"])
                                if emb is not None:
                                    insert_into_next_table(chunk["title"], chunk["content"], emb, "STAG Export")
                                    success_count += 1
                            except Exception as e:
                                log_sync_error("CSV", f"Chyba na ≈ô√°dku {idx}: {str(e)}")

                            update_sync_progress("CSV", idx)

                        print(f"‚úÖ CSV zpracov√°no: {total_rows} p≈ôedmƒõt≈Ø.")
                    else:
                        set_sync_status("CSV", "running", total=0)
                        log_sync_error("CSV", "Nelze naƒç√≠st obsah CSV.")
                except Exception as e:
                    log_sync_error("CSV", f"Chyba p≈ôi ƒçten√≠ CSV: {str(e)}")
                    print(f"‚ùå Chyba p≈ôi ƒçten√≠ CSV: {e}")
            else:
                set_sync_status("CSV", "running", total=0)
                log_sync_error("CSV", f"Soubor nenalezen: {csv_path}")
                print(f"‚ö†Ô∏è CSV soubor nenalezen na cestƒõ: {csv_path}. P≈ôeskoƒçeno.")

        # --- FIN√ÅLE: PROHOZEN√ç TABULEK ---
        print(f"üîÑ Prov√°d√≠m atomick√© prohozen√≠ tabulek (Zpracov√°no celkem {success_count} z√°znam≈Ø)...")
        swap_tables_atomic()

        if mode in ["all", "web"]: set_sync_status("WEB", "success")
        if mode in ["all", "csv"]: set_sync_status("CSV", "success")
        print("üéâ Indexace √∫spƒõ≈°nƒõ dokonƒçena. Data jsou LIVE.")

    except Exception as e:
        print(f"‚ùå Krizov√° chyba p≈ôi indexaci: {e}")
        if mode in ["all", "web"]:
            log_sync_error("WEB", f"Kritick√° chyba: {str(e)}")
            set_sync_status("WEB", "error")
        if mode in ["all", "csv"]:
            log_sync_error("CSV", f"Kritick√° chyba: {str(e)}")
            set_sync_status("CSV", "error")


if __name__ == "__main__":
    run_ingest("all")