import os
import json
import requests
import numpy as np
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import io
import docx
from pypdf import PdfReader
from config import OPENAI_API_KEY, GOOGLE_DRIVE_FOLDER_ID, GOOGLE_CREDENTIALS_FILE, EMBEDDING_MODEL, \
    OPENAI_EMBEDDING_URL
from database import insert_embedding_to_db, clear_database, init_db


# --- 1. P≈ôipojen√≠ ke Google Disku ---
def get_drive_service():
    if not os.path.exists(GOOGLE_CREDENTIALS_FILE):
        print(f"‚ùå Chyba: Soubor {GOOGLE_CREDENTIALS_FILE} nenalezen.")
        return None
    creds = service_account.Credentials.from_service_account_file(
        GOOGLE_CREDENTIALS_FILE, scopes=['https://www.googleapis.com/auth/drive.readonly'])
    return build('drive', 'v3', credentials=creds)


def process_file_content(service, file_item):
    print(f"  üìÑ Stahuji soubor: {file_item['name']}...")

    supported_types = ['application/pdf', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document']

    # Kontrola, zda jde o podporovan√Ω typ
    is_supported = file_item['mimeType'] in supported_types or file_item['name'].endswith(('.pdf', '.docx'))

    if not is_supported:
        return None

    try:
        request = service.files().get_media(fileId=file_item['id'])
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while done is False:
            status, done = downloader.next_chunk()

        fh.seek(0)
        text = ""

        if file_item['name'].endswith('.docx'):
            doc = docx.Document(fh)
            text = "\n".join([p.text for p in doc.paragraphs if p.text.strip() != ""])

        elif file_item['name'].endswith('.pdf'):
            reader = PdfReader(fh)
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"

        if text:
            return {"filename": file_item['name'], "text": text}

    except Exception as e:
        print(f"‚ö†Ô∏è Chyba p≈ôi stahov√°n√≠ {file_item['name']}: {e}")

    return None


def get_files_recursive(service, folder_id):
    results_list = []
    page_token = None

    while True:
        try:
            response = service.files().list(
                q=f"'{folder_id}' in parents and trashed = false",
                fields="nextPageToken, files(id, name, mimeType)",
                pageToken=page_token
            ).execute()
        except Exception as e:
            print(f"‚ö†Ô∏è Chyba p≈ôi listov√°n√≠ slo≈æky: {e}")
            break

        items = response.get('files', [])

        for item in items:
            if item['mimeType'] == 'application/vnd.google-apps.folder':
                print(f"üìÇ Vstupuji do podslo≈æky: {item['name']}")
                results_list.extend(get_files_recursive(service, item['id']))
            else:
                processed_file = process_file_content(service, item)
                if processed_file:
                    results_list.append(processed_file)

        page_token = response.get('nextPageToken')
        if not page_token:
            break

    return results_list


# --- 2. Inteligentn√≠ Chunking ---
def semantic_chunking(text, filename):
    print(f"üß† S√©mantick√© ≈ôez√°n√≠ souboru: {filename}...")

    # Fallback pro pr√°zdn√Ω text
    if not text or len(text.strip()) < 10:
        return []

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}

    # Zkr√°cen√≠ textu, pokud je moc dlouh√Ω (GPT limit)
    shortened_text = text[:12000]

    prompt = f"""
    Jsi expertn√≠ analytik. Rozdƒõl text na logick√© celky (chunky).
    Vstupn√≠ soubor: {filename}

    Pravidla:
    1. V√Ωstup MUS√ç b√Ωt validn√≠ JSON.
    2. Form√°t: {{"chunks": [ {{"title": "...", "content": "..."}} ]}}

    Text k anal√Ωze:
    {shortened_text}
    """

    data = {
        "model": "gpt-4o-mini",  # Pou≈æ√≠v√°me levnƒõj≈°√≠ model pro chunking
        "messages": [{"role": "user", "content": prompt}],
        "response_format": {"type": "json_object"}
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data)

        if response.status_code != 200:
            print(f"‚ö†Ô∏è API Error {response.status_code}: {response.text}")
            raise Exception("API call failed")

        result = response.json()
        content = result["choices"][0]["message"]["content"]
        json_content = json.loads(content)

        # R≈Øzn√© form√°ty, co m≈Ø≈æe AI vr√°tit (zaji≈°tƒõn√≠ kompatibility)
        if "chunks" in json_content:
            return json_content["chunks"]
        if "items" in json_content:
            return json_content["items"]
        if isinstance(json_content, list):
            return json_content

    except Exception as e:
        print(f"‚ö†Ô∏è Chyba AI chunkingu u {filename}: {e}. Pou≈æ√≠v√°m Fallback.")

    # --- Z√ÅCHRANN√Å BRZDA (FALLBACK) ---
    # Pokud cokoliv sel≈æe, vr√°t√≠me cel√Ω text jako jeden chunk.
    return [{"title": filename, "content": text}]


# --- 3. Embedding ---
def get_embedding(text):
    # Ochrana p≈ôed pr√°zdn√Ωm vstupem
    if not text or not text.strip():
        return None

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    data = {"input": text, "model": EMBEDDING_MODEL}

    try:
        response = requests.post(OPENAI_EMBEDDING_URL, headers=headers, json=data)
        if response.status_code == 200:
            return np.array(response.json()["data"][0]["embedding"])
        else:
            print(f"‚ö†Ô∏è Chyba Embeddings API: {response.text}")
    except Exception as e:
        print(f"‚ö†Ô∏è Chyba p≈ôi embeddingu: {e}")

    return None


# --- HLAVN√ç LOOP ---
if __name__ == "__main__":
    init_db()

    print("üöÄ Startuji indexaci Google Disku...")
    service = get_drive_service()

    if service:
        files_data = get_files_recursive(service, GOOGLE_DRIVE_FOLDER_ID)

        print(f"‚úÖ Nalezeno a sta≈æeno celkem {len(files_data)} soubor≈Ø.")

        if files_data:
            # Vyƒçist√≠me DB, a≈• tam nem√°me duplicity
            clear_database()
            print("üßπ Datab√°ze vyƒçi≈°tƒõna.")

            for i, file_item in enumerate(files_data):
                # Info o postupu
                print(f"[{i + 1}/{len(files_data)}] Zpracov√°v√°m: {file_item['filename']}")

                chunks = semantic_chunking(file_item['text'], file_item['filename'])

                # Kontrola proti NoneType erroru
                if not chunks:
                    print("   ‚ö†Ô∏è ≈Ω√°dn√© chunky nevr√°ceny, p≈ôeskakuji.")
                    continue

                for chunk in chunks:
                    title = chunk.get("title", file_item['filename'])  # Fallback na n√°zev souboru
                    text_content = chunk.get("content", "")

                    if text_content:
                        # --- ZDE JE TA KL√çƒåOV√Å ZMƒöNA: Obohacen√≠ kontextu ---
                        # Vytv√°≈ô√≠me "bohat√Ω text" jen pro v√Ωpoƒçet embeddingu (vektoru).
                        # Do datab√°ze ale ulo≈æ√≠me ƒçist√Ω text_content, aby se u≈æivateli zobrazoval hezky.
                        enriched_text_for_embedding = (
                            f"Zdrojov√Ω soubor: {file_item['filename']}\n"
                            f"T√©ma: {title}\n"
                            f"Obsah: {text_content}"
                        )

                        emb = get_embedding(enriched_text_for_embedding)

                        if emb is not None:
                            insert_embedding_to_db(title, text_content, emb, file_item['filename'])
                            print(f"   üíæ Ulo≈æeno: {title[:40]}...")

            print("üéâ Hotovo! V≈°echna data jsou v datab√°zi.")
        else:
            print("‚ö†Ô∏è ≈Ω√°dn√© relevantn√≠ soubory (PDF/DOCX) nenalezeny.")