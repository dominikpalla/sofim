import os
import json
import requests
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import io
from pypdf import PdfReader
import docx  # Ponech√°v√°me pro p≈ô√≠padn√Ω budouc√≠ lok√°ln√≠ DOCX import

from config import OPENAI_API_KEY, EMBEDDING_MODEL, OPENAI_EMBEDDING_URL
from database import (
    prepare_next_table_for_update,
    insert_into_next_table,
    swap_tables_atomic,
    get_db_connection,
    set_sync_status,
    update_sync_progress,
    log_sync_error
)


# --- 1. Pomocn√© funkce pro CRAWLER ---

def get_urls_from_db():
    """Naƒçte seznam URL k indexaci z datab√°ze."""
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT url FROM crawler_urls")
        urls = [row[0] for row in cursor.fetchall()]
    except Exception as e:
        print(f"‚ö†Ô∏è Tabulka crawler_urls asi neexistuje nebo je pr√°zdn√°: {e}")
        urls = []
    finally:
        conn.close()
    return urls


def scrape_uhk_page(url):
    """
    St√°hne str√°nku, najde PDF odkazy a po≈°le HTML do AI,
    aby inteligentnƒõ extrahovala jen ƒçist√Ω text bez balastu.
    """
    print(f"üï∏Ô∏è Crawluji: {url}")
    try:
        headers = {"User-Agent": "SofimBot/1.0 (UHK Internal)"}
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            print(f"   ‚ùå Chyba HTTP {response.status_code}")
            return None, []

        soup = BeautifulSoup(response.content, 'html.parser')

        # 1. Extrakce PDF odkaz≈Ø z cel√©ho DOMu
        pdf_urls = []
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            if '/file/' in href or href.lower().endswith('.pdf'):
                full_pdf_url = urljoin(url, href)
                if full_pdf_url not in pdf_urls:
                    pdf_urls.append(full_pdf_url)

        # 2. P≈ô√≠prava HTML pro LLM
        # Odstran√≠me tƒõ≈æk√Ω technick√Ω balast, abychom zbyteƒçnƒõ neplatili tokeny za Javascript a styly
        for element in soup(["script", "style", "noscript", "svg", "video", "iframe"]):
            element.decompose()

        # Vezmeme obsah tƒõla str√°nky (nebo cel√©, pokud body chyb√≠)
        html_for_ai = str(soup.body) if soup.body else str(soup)

        # 3. Nech√°me GPT vysekat ƒçist√Ω informaƒçn√≠ text
        print("   ü§ñ Deleguji extrakci textu z HTML na umƒõlou inteligenci...")
        llm_headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}

        prompt = f"""
        Jsi expertn√≠ extraktor dat. Tv√Ωm √∫kolem je z n√°sleduj√≠c√≠ho zdrojov√©ho k√≥du webov√© str√°nky vyt√°hnout POUZE hlavn√≠ informaƒçn√≠ obsah.
        Pravidla:
        1. Ignoruj ve≈°ker√© navigaƒçn√≠ prvky (hlavn√≠ menu), patiƒçky, hlaviƒçky univerzity, cookie li≈°ty a podobn√Ω balast.
        2. Ignoruj texty tlaƒç√≠tek nesouvisej√≠c√≠ s obsahem (nap≈ô. "Sd√≠let na Facebooku", "Zpƒõt na √∫vod", "Vyhledat").
        3. Vra≈• absolutnƒõ ƒçist√Ω text, kter√Ω nese informaƒçn√≠ hodnotu.
        4. Neodpov√≠dej ≈æ√°dn√Ωmi √∫vodn√≠mi fr√°zemi (jako "Zde je text:"), prostƒõ rovnou vypi≈° extrahovan√Ω obsah.

        Obsah webu:
        {html_for_ai[:60000]}
        """

        data = {
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.0  # Nulov√° teplota pro maxim√°ln√≠ vƒõcnost a nulov√© halucinace
        }

        llm_response = requests.post("https://api.openai.com/v1/chat/completions", headers=llm_headers, json=data)

        if llm_response.status_code == 200:
            clean_text = llm_response.json()["choices"][0]["message"]["content"].strip()

            # Pojistka: Pokud z toho AI nevy≈æd√≠mala skoro nic, asi to byla pr√°zdn√° str√°nka
            if len(clean_text) < 20:
                print("   ‚ö†Ô∏è AI z t√©to str√°nky nedostala ≈æ√°dn√Ω smyslupln√Ω text.")
                return None, pdf_urls

            return clean_text, pdf_urls
        else:
            print(f"   ‚ö†Ô∏è Chyba API p≈ôi extrakci HTML: {llm_response.text}")
            return None, pdf_urls

    except Exception as e:
        print(f"‚ö†Ô∏è Chyba p≈ôi stahov√°n√≠/zpracov√°n√≠ {url}: {e}")
        return None, []


def process_pdf_from_url(pdf_url):
    """St√°hne a p≈ôeƒçte PDF z URL do pamƒõti."""
    print(f"   üìÑ Stahuji PDF: {pdf_url}")
    try:
        headers = {"User-Agent": "SofimBot/1.0 (UHK Internal)"}
        response = requests.get(pdf_url, headers=headers, timeout=15)

        if response.status_code == 200:
            fh = io.BytesIO(response.content)
            reader = PdfReader(fh)
            text = ""
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"

            # Validace, zda to nen√≠ jen sken (obr√°zek)
            if len(text.strip()) < 10:
                print(f"   ‚ö†Ô∏è PDF {pdf_url} je pravdƒõpodobnƒõ sken bez textov√© vrstvy.")
                return None

            return text
        else:
            print(f"   ‚ùå Nelze st√°hnout PDF (Status {response.status_code})")
    except Exception as e:
        print(f"   ‚ùå Chyba ƒçten√≠ PDF {pdf_url}: {e}")
    return None


# --- 2. Pomocn√© funkce pro CSV (Hybridn√≠ model) ---

def read_csv_smart(fh):
    """Naƒçte CSV s d≈Ørazem na zachov√°n√≠ v≈°ech dat, porad√≠ si s k√≥dov√°n√≠m i oddƒõlovaƒçi."""
    encodings = ['utf-8', 'cp1250', 'latin1']

    for encoding in encodings:
        fh.seek(0)
        try:
            df = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, on_bad_lines='skip')
            keywords = ['zkratka', 'zkr_predm', 'nazev_cz', 'kredity', 'anotace_cz']

            col_str = str(list(df.columns)).lower()
            if not any(k in col_str for k in keywords):
                fh.seek(0)
                df_raw = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, header=None, on_bad_lines='skip',
                                     nrows=15)

                header_index = -1
                for i in range(len(df_raw)):
                    row_str = str(df_raw.iloc[i].values).lower()
                    if any(k in row_str for k in keywords):
                        header_index = i
                        break

                if header_index != -1:
                    fh.seek(0)
                    df = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, header=header_index,
                                     on_bad_lines='skip')

            df = df.dropna(how='all')
            df = df.fillna("")
            df.columns = [str(c).strip() for c in df.columns]
            return df

        except Exception:
            continue
    return None


# --- 3. Chunking funkce ---

def semantic_chunking(text, filename):
    """
    Inteligentn√≠ ≈ôez√°n√≠ textu pomoc√≠ GPT-4o-mini.
    PLN√Å PODPORA FULLTEXTU - Dlouh√© texty rozdƒõl√≠ na bloky a pro≈æene AI v cyklu.
    """
    if not text or len(text.strip()) < 10:
        return []

    print(f"üß† S√©mantick√© ≈ôez√°n√≠ obsahu: {filename} ({len(text)} znak≈Ø)...")
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}

    # Rozsek√°me cel√Ω dokument na bloky po cca 12000 znac√≠ch (aby nedo≈°lo k limitu token≈Ø na jeden dotaz)
    chunk_size = 12000
    text_blocks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    all_extracted_chunks = []

    for idx, block in enumerate(text_blocks):
        if len(text_blocks) > 1:
            print(f"   ‚è≥ Zpracov√°v√°m ƒç√°st {idx + 1}/{len(text_blocks)}...")

        prompt = f"""
        Jsi expertn√≠ analytik. Rozdƒõl text na logick√© celky (chunky).
        Zdroj: {filename} (ƒå√°st {idx + 1} z {len(text_blocks)})
        Pravidla:
        1. V√Ωstup MUS√ç b√Ωt validn√≠ JSON.
        2. Form√°t: {{"chunks": [ {{"title": "...", "content": "..."}} ]}}
        Text k anal√Ωze:
        {block}
        """

        data = {
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": prompt}],
            "response_format": {"type": "json_object"},
            "temperature": 0.0
        }

        try:
            response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data)
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                json_content = json.loads(content)

                if "chunks" in json_content:
                    all_extracted_chunks.extend(json_content["chunks"])
                elif "items" in json_content:
                    all_extracted_chunks.extend(json_content["items"])
            else:
                print(f"   ‚ö†Ô∏è API Error u ƒç√°sti {idx + 1}: {response.text}")
        except Exception as e:
            print(f"‚ö†Ô∏è Chyba AI chunkingu u ƒç√°sti {idx + 1}: {e}.")

    # Fallback, kdyby n√°hodou API tot√°lnƒõ selhalo u v≈°ech ƒç√°st√≠
    if not all_extracted_chunks:
        print("   ‚ö†Ô∏è S√©mantick√Ω chunking zcela selhal, pou≈æ√≠v√°m hrub√Ω fallback.")
        return [{"title": f"Obsah z {filename}", "content": text[:10000]}]

    return all_extracted_chunks


def csv_row_chunking(df, filename):
    """≈ò√°dkov√© zpracov√°n√≠ tabulky p≈ôedmƒõt≈Ø."""
    print(f"üìä Zpracov√°v√°m tabulku p≈ôedmƒõt≈Ø: {filename} ({len(df)} ≈ô√°dk≈Ø)...")
    chunks = []

    for index, row in df.iterrows():
        row_dict = row.to_dict()

        nazev = row_dict.get('NAZEV_CZ', row_dict.get('NAZEV_AN', 'Nezn√°m√Ω p≈ôedmƒõt'))
        kod = row_dict.get('ZKR_PREDM', '')

        if not kod:
            for k, v in row_dict.items():
                if 'zkr' in str(k).lower() and not kod: kod = str(v)

        if nazev == 'Nezn√°m√Ω p≈ôedmƒõt' and not kod:
            continue

        title = f"P≈ôedmƒõt: {nazev} ({kod})".strip()
        content_lines = [f"--- Detail p≈ôedmƒõtu: {title} ---"]

        priority_fields = {
            'NAZEV_AN': 'Anglick√Ω n√°zev', 'GARANTI': 'Garanti', 'VYUCUJICI': 'Vyuƒçuj√≠c√≠',
            'KREDITY': 'Kredity', 'ROK_VARIANTY': 'Rok varianty', 'ANOTACE_CZ': 'Anotace',
            'CIL_CZ': 'C√≠le p≈ôedmƒõtu', 'OSNOVA_CZ': 'Osnova', 'LITERATURA': 'Literatura',
            'POZADAVKY_CZ': 'Po≈æadavky', 'METODY_VYUKY_CZ': 'Metody', 'URL': 'Odkaz'
        }

        for key, label in priority_fields.items():
            if key in row_dict:
                val = str(row_dict[key]).strip()
                if val and val.lower() != 'nan':
                    content_lines.append(f"{label}: {val}")

        chunks.append({"title": title, "content": "\n".join(content_lines)})

    return chunks


# --- 4. Embedding ---

def get_embedding(text):
    if not text or not text.strip():
        return None

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    data = {"input": text, "model": EMBEDDING_MODEL}

    try:
        response = requests.post(OPENAI_EMBEDDING_URL, headers=headers, json=data)
        if response.status_code == 200:
            return np.array(response.json()["data"][0]["embedding"])
    except Exception:
        pass
    return None


# --- 5. HLAVN√ç LOGIKA INDEXACE ---

def run_ingest(mode="all"):
    """
    Spust√≠ proces ingestu. Re≈æimy: 'all', 'web', 'csv'.
    Propojeno s datab√°z√≠ pro sledov√°n√≠ pr≈Øbƒõhu v admin panelu.
    """
    print(f"üöÄ Startuji indexaci na pozad√≠ (Re≈æim: {mode})...")

    if mode in ["all", "web"]: set_sync_status("WEB", "running")
    if mode in ["all", "csv"]: set_sync_status("CSV", "running")

    try:
        prepare_next_table_for_update(mode)
        success_count = 0

        # --- F√ÅZE A: CRAWLER (Web UHK) ---
        if mode in ["all", "web"]:
            urls = get_urls_from_db()
            total_urls = len(urls)

            set_sync_status("WEB", "running", total=total_urls)

            if urls:
                print(f"üåç Nalezeno {total_urls} URL adres k indexaci.")
                for idx, url in enumerate(urls, 1):
                    try:
                        web_text, pdf_links = scrape_uhk_page(url)

                        if web_text:
                            chunks = semantic_chunking(web_text, f"Web: {url}")
                            for chunk in chunks:
                                title = chunk.get("title", "Webov√° str√°nka")
                                content = chunk.get("content", "")
                                emb = get_embedding(f"URL: {url}\n{content}")
                                if emb is not None:
                                    insert_into_next_table(title, content, emb, url)
                                    print(f"   üíæ Web ulo≈æen: {title[:30]}...")
                                    success_count += 1

                        if pdf_links:
                            print(f"   üìé Nalezeno {len(pdf_links)} PDF dokument≈Ø na odkazu {url}.")
                            for pdf_url in pdf_links:
                                pdf_text = process_pdf_from_url(pdf_url)
                                if pdf_text:
                                    chunks = semantic_chunking(pdf_text, f"PDF: {pdf_url.split('/')[-1]}")
                                    for chunk in chunks:
                                        title = chunk.get("title", "PDF Dokument")
                                        content = chunk.get("content", "")
                                        emb = get_embedding(f"Zdroj PDF: {pdf_url}\n{content}")
                                        if emb is not None:
                                            insert_into_next_table(title, content, emb, pdf_url)
                                            success_count += 1

                    except Exception as e:
                        log_sync_error("WEB", f"Chyba na {url}: {str(e)}")
                        print(f"   ‚ùå Chyba zpracov√°n√≠ {url}: {e}")

                    update_sync_progress("WEB", idx)
            else:
                print("‚ö†Ô∏è ≈Ω√°dn√° URL v datab√°zi. P≈ôidej je p≈ôes /admin.")

        # --- F√ÅZE B: LOK√ÅLN√ç CSV (Studijn√≠ pl√°ny) ---
        if mode in ["all", "csv"]:
            csv_path = "data/predmety.csv"

            if os.path.exists(csv_path):
                print(f"üìä Naƒç√≠t√°m lok√°ln√≠ CSV: {csv_path}")
                try:
                    with open(csv_path, "rb") as f:
                        df = read_csv_smart(f)

                    if df is not None:
                        csv_chunks = csv_row_chunking(df, "Lok√°ln√≠ Datab√°ze P≈ôedmƒõt≈Ø")
                        total_rows = len(csv_chunks)

                        set_sync_status("CSV", "running", total=total_rows)

                        for idx, chunk in enumerate(csv_chunks, 1):
                            emb = get_embedding(chunk["content"])
                            if emb is not None:
                                insert_into_next_table(chunk["title"], chunk["content"], emb, "STAG Export")
                                success_count += 1

                            update_sync_progress("CSV", idx)

                        print(f"‚úÖ CSV zpracov√°no: {total_rows} p≈ôedmƒõt≈Ø.")
                    else:
                        set_sync_status("CSV", "running", total=0)
                        log_sync_error("CSV", "Nelze naƒç√≠st obsah CSV.")
                except Exception as e:
                    log_sync_error("CSV", f"Chyba p≈ôi ƒçten√≠ CSV: {str(e)}")
                    print(f"‚ùå Chyba p≈ôi ƒçten√≠ CSV: {e}")
            else:
                set_sync_status("CSV", "running", total=0)
                log_sync_error("CSV", f"Soubor nenalezen: {csv_path}")
                print(f"‚ö†Ô∏è CSV soubor nenalezen na cestƒõ: {csv_path}. P≈ôeskoƒçeno.")

        # --- FIN√ÅLE: PROHOZEN√ç TABULEK ---
        print(f"üîÑ Prov√°d√≠m atomick√© prohozen√≠ tabulek (Zpracov√°no celkem {success_count} z√°znam≈Ø)...")
        swap_tables_atomic()

        if mode in ["all", "web"]: set_sync_status("WEB", "success")
        if mode in ["all", "csv"]: set_sync_status("CSV", "success")
        print("üéâ Indexace √∫spƒõ≈°nƒõ dokonƒçena. Data jsou LIVE.")

    except Exception as e:
        print(f"‚ùå Krizov√° chyba p≈ôi indexaci: {e}")
        if mode in ["all", "web"]:
            log_sync_error("WEB", f"Kritick√° chyba: {str(e)}")
            set_sync_status("WEB", "error")
        if mode in ["all", "csv"]:
            log_sync_error("CSV", f"Kritick√° chyba: {str(e)}")
            set_sync_status("CSV", "error")


if __name__ == "__main__":
    run_ingest("all")