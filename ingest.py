import os
import json
import requests
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import io
from pypdf import PdfReader
import docx  # Ponech√°v√°me, kdybychom v budoucnu chtƒõli importovat lok√°ln√≠ DOCX

from config import OPENAI_API_KEY, EMBEDDING_MODEL, OPENAI_EMBEDDING_URL
from database import (
    prepare_next_table_for_update,
    insert_into_next_table,
    swap_tables_atomic,
    get_db_connection,
    set_sync_status,
    update_sync_progress,
    log_sync_error
)


# --- 1. Pomocn√© funkce pro CRAWLER ---

def get_urls_from_db():
    """Naƒçte seznam URL k indexaci z datab√°ze."""
    conn = get_db_connection()
    cursor = conn.cursor()
    # Ovƒõ≈ô√≠me, zda tabulka existuje (pro jistotu)
    try:
        cursor.execute("SELECT url FROM crawler_urls")
        urls = [row[0] for row in cursor.fetchall()]
    except Exception as e:
        print(f"‚ö†Ô∏è Tabulka crawler_urls asi neexistuje nebo je pr√°zdn√°: {e}")
        urls = []
    finally:
        conn.close()
    return urls


def scrape_uhk_page(url):
    """St√°hne str√°nku, vyƒçist√≠ HTML a najde PDF odkazy."""
    print(f"üï∏Ô∏è Crawluji: {url}")
    try:
        headers = {"User-Agent": "SofimBot/1.0 (UHK Internal)"}
        response = requests.get(url, headers=headers, timeout=10)

        if response.status_code != 200:
            print(f"   ‚ùå Chyba HTTP {response.status_code}")
            return None, []

        soup = BeautifulSoup(response.content, 'html.parser')

        # 1. Hled√°n√≠ PDF odkaz≈Ø P≈òEDT√çM, ne≈æ proma≈æeme DOM
        pdf_urls = []
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            # UHK ukl√°d√° soubory ƒçasto p≈ôes /file/ nebo konƒç√≠ .pdf
            if '/file/' in href or href.lower().endswith('.pdf'):
                full_pdf_url = urljoin(url, href)
                if full_pdf_url not in pdf_urls:
                    pdf_urls.append(full_pdf_url)

        # 2. Agresivn√≠ ƒçi≈°tƒõn√≠ balastu
        for element in soup(["header", "footer", "nav", "script", "style", "noscript", "iframe"]):
            element.decompose()

        # Zac√≠len√≠ na UHK specifick√© t≈ô√≠dy
        main_content = soup.find(class_="main__content") or soup.find("main") or soup.find("article")
        target_soup = main_content if main_content else soup.body

        if not target_soup:
            return None, pdf_urls

        # Odstranƒõn√≠ dal≈°√≠ho balastu
        for noise in target_soup.find_all(class_=["share-buttons", "sidebar", "breadcrumb", "cookies-bar"]):
            noise.decompose()

        # 3. Extrakce ƒçist√©ho textu
        raw_text = target_soup.get_text(separator='\n', strip=True)
        clean_text = "\n".join([line.strip() for line in raw_text.splitlines() if line.strip()])

        return clean_text, pdf_urls

    except Exception as e:
        print(f"‚ö†Ô∏è Chyba p≈ôi stahov√°n√≠ {url}: {e}")
        return None, []


def process_pdf_from_url(pdf_url):
    """St√°hne a p≈ôeƒçte PDF z URL do pamƒõti."""
    print(f"   üìÑ Stahuji PDF: {pdf_url}")
    try:
        headers = {"User-Agent": "SofimBot/1.0 (UHK Internal)"}
        response = requests.get(pdf_url, headers=headers, timeout=15)

        if response.status_code == 200:
            fh = io.BytesIO(response.content)
            reader = PdfReader(fh)
            text = ""
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"

            # Validace, zda to nen√≠ jen sken (obr√°zek)
            if len(text.strip()) < 10:
                print(f"   ‚ö†Ô∏è PDF {pdf_url} je pravdƒõpodobnƒõ sken bez textov√© vrstvy.")
                return None

            return text
        else:
            print(f"   ‚ùå Nelze st√°hnout PDF (Status {response.status_code})")
    except Exception as e:
        print(f"   ‚ùå Chyba ƒçten√≠ PDF {pdf_url}: {e}")
    return None


# --- 2. Pomocn√© funkce pro CSV (Hybridn√≠ model) ---

def read_csv_smart(fh):
    """Naƒçte CSV s d≈Ørazem na zachov√°n√≠ v≈°ech dat, porad√≠ si s k√≥dov√°n√≠m i oddƒõlovaƒçi."""
    encodings = ['utf-8', 'cp1250', 'latin1']

    for encoding in encodings:
        fh.seek(0)
        try:
            # P≈ôeƒçteme CSV
            df = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, on_bad_lines='skip')

            # Validace hlaviƒçky podle kl√≠ƒçov√Ωch slov
            keywords = ['zkratka', 'zkr_predm', 'nazev_cz', 'kredity', 'anotace_cz']

            # Pokud hlaviƒçka nesed√≠, zkus√≠me ji naj√≠t n√≠≈æe
            col_str = str(list(df.columns)).lower()
            if not any(k in col_str for k in keywords):
                fh.seek(0)
                df_raw = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, header=None, on_bad_lines='skip',
                                     nrows=15)

                header_index = -1
                for i in range(len(df_raw)):
                    row_str = str(df_raw.iloc[i].values).lower()
                    if any(k in row_str for k in keywords):
                        header_index = i
                        break

                if header_index != -1:
                    fh.seek(0)
                    df = pd.read_csv(fh, sep=None, engine='python', encoding=encoding, header=header_index,
                                     on_bad_lines='skip')

            # Vyƒçi≈°tƒõn√≠
            df = df.dropna(how='all')
            df = df.fillna("")
            df.columns = [str(c).strip() for c in df.columns]

            return df

        except Exception:
            continue
    return None


# --- 3. Chunking funkce (Nezmƒõnƒõno) ---

def semantic_chunking(text, filename):
    """Inteligentn√≠ ≈ôez√°n√≠ textu pomoc√≠ GPT-4o-mini."""
    if not text or len(text.strip()) < 10:
        return []

    print(f"üß† S√©mantick√© ≈ôez√°n√≠ obsahu: {filename}...")

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    shortened_text = text[:12000]  # Limit token≈Ø

    prompt = f"""
    Jsi expertn√≠ analytik. Rozdƒõl text na logick√© celky (chunky).
    Zdroj: {filename}
    Pravidla:
    1. V√Ωstup MUS√ç b√Ωt validn√≠ JSON.
    2. Form√°t: {{"chunks": [ {{"title": "...", "content": "..."}} ]}}
    Text k anal√Ωze:
    {shortened_text}
    """

    data = {
        "model": "gpt-4o-mini",  # Levn√Ω model na chunking
        "messages": [{"role": "user", "content": prompt}],
        "response_format": {"type": "json_object"}
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data)
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            json_content = json.loads(content)

            if "chunks" in json_content: return json_content["chunks"]
            if "items" in json_content: return json_content["items"]
    except Exception as e:
        print(f"‚ö†Ô∏è Chyba AI chunkingu: {e}. Pou≈æ√≠v√°m Fallback.")

    # Fallback: Vr√°t√≠me to jako jeden kus
    return [{"title": f"Obsah z {filename}", "content": text}]


def csv_row_chunking(df, filename):
    """≈ò√°dkov√© zpracov√°n√≠ tabulky p≈ôedmƒõt≈Ø."""
    print(f"üìä Zpracov√°v√°m tabulku p≈ôedmƒõt≈Ø: {filename} ({len(df)} ≈ô√°dk≈Ø)...")
    chunks = []

    for index, row in df.iterrows():
        row_dict = row.to_dict()

        # Identifikace
        nazev = row_dict.get('NAZEV_CZ', row_dict.get('NAZEV_AN', 'Nezn√°m√Ω p≈ôedmƒõt'))
        kod = row_dict.get('ZKR_PREDM', '')

        # Hled√°n√≠ k√≥du jinde
        if not kod:
            for k, v in row_dict.items():
                if 'zkr' in str(k).lower() and not kod: kod = str(v)

        if nazev == 'Nezn√°m√Ω p≈ôedmƒõt' and not kod:
            continue

        title = f"P≈ôedmƒõt: {nazev} ({kod})".strip()
        content_lines = [f"--- Detail p≈ôedmƒõtu: {title} ---"]

        priority_fields = {
            'NAZEV_AN': 'Anglick√Ω n√°zev', 'GARANTI': 'Garanti', 'VYUCUJICI': 'Vyuƒçuj√≠c√≠',
            'KREDITY': 'Kredity', 'ROK_VARIANTY': 'Rok varianty', 'ANOTACE_CZ': 'Anotace',
            'CIL_CZ': 'C√≠le p≈ôedmƒõtu', 'OSNOVA_CZ': 'Osnova', 'LITERATURA': 'Literatura',
            'POZADAVKY_CZ': 'Po≈æadavky', 'METODY_VYUKY_CZ': 'Metody', 'URL': 'Odkaz'
        }

        for key, label in priority_fields.items():
            if key in row_dict:
                val = str(row_dict[key]).strip()
                if val and val.lower() != 'nan':
                    content_lines.append(f"{label}: {val}")

        chunks.append({"title": title, "content": "\n".join(content_lines)})

    return chunks


# --- 4. Embedding (Nezmƒõnƒõno) ---

def get_embedding(text):
    if not text or not text.strip():
        return None

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    data = {"input": text, "model": EMBEDDING_MODEL}

    try:
        response = requests.post(OPENAI_EMBEDDING_URL, headers=headers, json=data)
        if response.status_code == 200:
            return np.array(response.json()["data"][0]["embedding"])
    except Exception:
        pass
    return None


# --- 5. HLAVN√ç LOGIKA INDEXACE ---

def run_ingest(mode="all"):
    """
    Spust√≠ proces ingestu. Re≈æimy: 'all', 'web', 'csv'.
    Propojeno s datab√°z√≠ pro sledov√°n√≠ pr≈Øbƒõhu v admin panelu.
    """
    print(f"üöÄ Startuji indexaci na pozad√≠ (Re≈æim: {mode})...")

    # Nastav√≠me status v DB na "bƒõ≈æ√≠" (zat√≠m bez celkov√©ho poƒçtu, ten se updatne hned jak ho zjist√≠me)
    if mode in ["all", "web"]: set_sync_status("WEB", "running")
    if mode in ["all", "csv"]: set_sync_status("CSV", "running")

    try:
        # P≈ôiprav√≠me st√≠novou tabulku (vyƒçist√≠ v≈°e / zkop√≠ruje a p≈ôiprav√≠ pro ƒç√°steƒçn√Ω update podle m√≥du)
        prepare_next_table_for_update(mode)
        success_count = 0

        # --- F√ÅZE A: CRAWLER (Web UHK) ---
        if mode in ["all", "web"]:
            urls = get_urls_from_db()
            total_urls = len(urls)

            # Nastav√≠me celkov√Ω poƒçet URL do datab√°ze pro progress bar
            set_sync_status("WEB", "running", total=total_urls)

            if urls:
                print(f"üåç Nalezeno {total_urls} URL adres k indexaci.")
                for idx, url in enumerate(urls, 1):
                    try:
                        web_text, pdf_links = scrape_uhk_page(url)

                        if web_text:
                            chunks = semantic_chunking(web_text, f"Web: {url}")
                            for chunk in chunks:
                                title = chunk.get("title", "Webov√° str√°nka")
                                content = chunk.get("content", "")
                                emb = get_embedding(f"URL: {url}\n{content}")
                                if emb is not None:
                                    insert_into_next_table(title, content, emb, url)
                                    print(f"   üíæ Web ulo≈æen: {title[:30]}...")
                                    success_count += 1

                        if pdf_links:
                            print(f"   üìé Nalezeno {len(pdf_links)} PDF dokument≈Ø na odkazu {url}.")
                            for pdf_url in pdf_links:
                                pdf_text = process_pdf_from_url(pdf_url)
                                if pdf_text:
                                    chunks = semantic_chunking(pdf_text, f"PDF: {pdf_url.split('/')[-1]}")
                                    for chunk in chunks:
                                        title = chunk.get("title", "PDF Dokument")
                                        content = chunk.get("content", "")
                                        emb = get_embedding(f"Zdroj PDF: {pdf_url}\n{content}")
                                        if emb is not None:
                                            insert_into_next_table(title, content, emb, pdf_url)
                                            success_count += 1

                    except Exception as e:
                        log_sync_error("WEB", f"Chyba na {url}: {str(e)}")
                        print(f"   ‚ùå Chyba zpracov√°n√≠ {url}: {e}")

                    # üì¢ Pr≈Øbƒõ≈æn√Ω report postupu do datab√°ze
                    update_sync_progress("WEB", idx)
            else:
                print("‚ö†Ô∏è ≈Ω√°dn√° URL v datab√°zi. P≈ôidej je p≈ôes /admin.")

        # --- F√ÅZE B: LOK√ÅLN√ç CSV (Studijn√≠ pl√°ny) ---
        if mode in ["all", "csv"]:
            csv_path = "data/predmety.csv"

            if os.path.exists(csv_path):
                print(f"üìä Naƒç√≠t√°m lok√°ln√≠ CSV: {csv_path}")
                try:
                    with open(csv_path, "rb") as f:
                        df = read_csv_smart(f)

                    if df is not None:
                        csv_chunks = csv_row_chunking(df, "Lok√°ln√≠ Datab√°ze P≈ôedmƒõt≈Ø")
                        total_rows = len(csv_chunks)

                        # Nastav√≠me celkov√Ω poƒçet pro progress bar
                        set_sync_status("CSV", "running", total=total_rows)

                        for idx, chunk in enumerate(csv_chunks, 1):
                            emb = get_embedding(chunk["content"])
                            if emb is not None:
                                # Kl√≠ƒçov√©: Udr≈æ√≠me identifik√°tor "STAG Export" pro parci√°ln√≠ maz√°n√≠
                                insert_into_next_table(chunk["title"], chunk["content"], emb, "STAG Export")
                                success_count += 1

                            # üì¢ Pr≈Øbƒõ≈æn√Ω report postupu
                            update_sync_progress("CSV", idx)

                        print(f"‚úÖ CSV zpracov√°no: {total_rows} p≈ôedmƒõt≈Ø.")
                    else:
                        set_sync_status("CSV", "running", total=0)
                        log_sync_error("CSV", "Nelze naƒç√≠st obsah CSV.")
                except Exception as e:
                    log_sync_error("CSV", f"Chyba p≈ôi ƒçten√≠ CSV: {str(e)}")
                    print(f"‚ùå Chyba p≈ôi ƒçten√≠ CSV: {e}")
            else:
                set_sync_status("CSV", "running", total=0)
                log_sync_error("CSV", f"Soubor nenalezen: {csv_path}")
                print(f"‚ö†Ô∏è CSV soubor nenalezen na cestƒõ: {csv_path}. P≈ôeskoƒçeno.")

        # --- FIN√ÅLE: PROHOZEN√ç TABULEK ---
        print(f"üîÑ Prov√°d√≠m atomick√© prohozen√≠ tabulek (Zpracov√°no celkem {success_count} z√°znam≈Ø)...")
        # Prohod√≠me tabulky i kdyby success_count byl 0 (nap≈ô. p≈ôi smaz√°n√≠ url se mus√≠ live db aktualizovat)
        swap_tables_atomic()

        # Nastav√≠me status na √∫spƒõch a z√≠sk√°me hezk√Ω timestamp aktu√°ln√≠ho ƒçasu
        if mode in ["all", "web"]: set_sync_status("WEB", "success")
        if mode in ["all", "csv"]: set_sync_status("CSV", "success")
        print("üéâ Indexace √∫spƒõ≈°nƒõ dokonƒçena. Data jsou LIVE.")

    except Exception as e:
        print(f"‚ùå Krizov√° chyba p≈ôi indexaci: {e}")
        # P≈ôi krizov√© chybƒõ to zalogujeme a hod√≠me do stavu error/idle
        if mode in ["all", "web"]:
            log_sync_error("WEB", f"Kritick√° chyba: {str(e)}")
            set_sync_status("WEB", "error")
        if mode in ["all", "csv"]:
            log_sync_error("CSV", f"Kritick√° chyba: {str(e)}")
            set_sync_status("CSV", "error")


if __name__ == "__main__":
    # Pokud spust√≠≈° ingest.py ruƒçnƒõ z konzole, spust√≠ se kompletn√≠ indexace
    run_ingest("all")