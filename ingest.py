import os
import json
import requests
import numpy as np
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import io
import docx
from pypdf import PdfReader
from config import OPENAI_API_KEY, GOOGLE_DRIVE_FOLDER_ID, GOOGLE_CREDENTIALS_FILE, EMBEDDING_MODEL, \
    OPENAI_EMBEDDING_URL
from database import insert_embedding_to_db, clear_database, init_db


# --- 1. P≈ôipojen√≠ ke Google Disku ---
def get_drive_service():
    if not os.path.exists(GOOGLE_CREDENTIALS_FILE):
        print(f"‚ùå Chyba: Soubor {GOOGLE_CREDENTIALS_FILE} nenalezen.")
        return None
    creds = service_account.Credentials.from_service_account_file(
        GOOGLE_CREDENTIALS_FILE, scopes=['https://www.googleapis.com/auth/drive.readonly'])
    return build('drive', 'v3', credentials=creds)


def process_file_content(service, file_item):
    print(f"  üìÑ Stahuji soubor: {file_item['name']}...")

    # Podporovan√© typy + CSV
    supported_types = [
        'application/pdf',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'text/csv',
        'application/vnd.ms-excel'  # Nƒõkdy se CSV tv√°≈ô√≠ jako Excel
    ]

    is_supported = file_item['mimeType'] in supported_types or file_item['name'].endswith(('.pdf', '.docx', '.csv'))

    if not is_supported:
        return None

    try:
        request = service.files().get_media(fileId=file_item['id'])
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while done is False:
            status, done = downloader.next_chunk()

        fh.seek(0)

        # --- ZPRACOV√ÅN√ç CSV (P≈òEDMƒöTY) ---
        if file_item['name'].endswith('.csv'):
            try:
                # Naƒçteme CSV pomoc√≠ Pandas (zvl√°dne r≈Øzn√© k√≥dov√°n√≠ i oddƒõlovaƒçe)
                # Zkus√≠me detekovat oddƒõlovaƒç, nebo defaultnƒõ ƒç√°rku/st≈ôedn√≠k
                # Prvn√≠ pokus: UTF-8
                try:
                    df = pd.read_csv(fh, encoding='utf-8', on_bad_lines='skip')
                except:
                    # Druh√Ω pokus: Windows-1250 (ƒçesk√©) a st≈ôedn√≠k
                    fh.seek(0)
                    df = pd.read_csv(fh, sep=';', encoding='cp1250', on_bad_lines='skip')

                # Nahrad√≠me NaN za pr√°zdn√© stringy
                df = df.fillna("")

                # Vr√°t√≠me DataFrame p≈ô√≠mo, ne text
                return {"filename": file_item['name'], "type": "csv", "data": df}

            except Exception as e:
                print(f"   ‚ùå Chyba ƒçten√≠ CSV {file_item['name']}: {e}")
                return None

        # --- ZPRACOV√ÅN√ç DOCX ---
        text = ""
        if file_item['name'].endswith('.docx'):
            try:
                doc = docx.Document(fh)
                text = "\n".join([p.text for p in doc.paragraphs if p.text.strip() != ""])
            except Exception as e:
                print(f"   ‚ùå Chyba ƒçten√≠ DOCX {file_item['name']}: {e}")

        # --- ZPRACOV√ÅN√ç PDF ---
        elif file_item['name'].endswith('.pdf'):
            try:
                reader = PdfReader(fh)
                count = 0
                for page in reader.pages:
                    extracted = page.extract_text()
                    if extracted:
                        text += extracted + "\n"
                        count += 1

                # Detekce "pr√°zdn√©ho" PDF (sken)
                if len(text.strip()) == 0 and count > 0:
                    print(f"   ‚ö†Ô∏è PDF {file_item['name']} m√° str√°nky, ale ≈æ√°dn√Ω text. Asi sken?")
            except Exception as e:
                print(f"   ‚ùå Chyba ƒçten√≠ PDF {file_item['name']}: {e}")

        # Pokud se poda≈ôilo naƒç√≠st text z dokumentu
        if text:
            # Kontrola d√©lky textu
            text_len = len(text.strip())
            if text_len < 10:
                print(f"   ‚ö†Ô∏è VAROV√ÅN√ç: Soubor {file_item['name']} obsahuje jen {text_len} znak≈Ø! (Ignoruji)")
                return None

            return {"filename": file_item['name'], "type": "text", "text": text}

    except Exception as e:
        print(f"‚ö†Ô∏è Chyba p≈ôi stahov√°n√≠ {file_item['name']}: {e}")

    return None


def get_files_recursive(service, folder_id):
    results_list = []
    page_token = None

    while True:
        try:
            response = service.files().list(
                q=f"'{folder_id}' in parents and trashed = false",
                fields="nextPageToken, files(id, name, mimeType)",
                pageToken=page_token
            ).execute()
        except Exception as e:
            print(f"‚ö†Ô∏è Chyba p≈ôi listov√°n√≠ slo≈æky: {e}")
            break

        items = response.get('files', [])

        for item in items:
            if item['mimeType'] == 'application/vnd.google-apps.folder':
                print(f"üìÇ Vstupuji do podslo≈æky: {item['name']}")
                results_list.extend(get_files_recursive(service, item['id']))
            else:
                processed_file = process_file_content(service, item)
                if processed_file:
                    results_list.append(processed_file)

        page_token = response.get('nextPageToken')
        if not page_token:
            break

    return results_list


# --- 2. Chunking funkce ---

# A) S√©mantick√© ≈ôez√°n√≠ pro dokumenty (PDF/DOCX)
def semantic_chunking(text, filename):
    print(f"üß† S√©mantick√© ≈ôez√°n√≠ souboru: {filename}...")

    if not text or len(text.strip()) < 10:
        return []

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}

    # Zkr√°cen√≠ textu, pokud je moc dlouh√Ω (GPT limit)
    shortened_text = text[:12000]

    prompt = f"""
    Jsi expertn√≠ analytik. Rozdƒõl text na logick√© celky (chunky).
    Vstupn√≠ soubor: {filename}

    Pravidla:
    1. V√Ωstup MUS√ç b√Ωt validn√≠ JSON.
    2. Form√°t: {{"chunks": [ {{"title": "...", "content": "..."}} ]}}

    Text k anal√Ωze:
    {shortened_text}
    """

    data = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": prompt}],
        "response_format": {"type": "json_object"}
    }

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=data)

        if response.status_code != 200:
            print(f"‚ö†Ô∏è API Error {response.status_code}: {response.text}")
            raise Exception("API call failed")

        result = response.json()
        content = result["choices"][0]["message"]["content"]
        json_content = json.loads(content)

        if "chunks" in json_content: return json_content["chunks"]
        if "items" in json_content: return json_content["items"]
        if isinstance(json_content, list): return json_content

    except Exception as e:
        print(f"‚ö†Ô∏è Chyba AI chunkingu u {filename}: {e}. Pou≈æ√≠v√°m Fallback.")

    # Fallback: vr√°t√≠ cel√Ω text jako jeden chunk
    return [{"title": filename, "content": text}]


# B) ≈ò√°dkov√© ≈ôez√°n√≠ pro tabulky (CSV)
def csv_row_chunking(df, filename):
    print(f"üìä Zpracov√°v√°m tabulku p≈ôedmƒõt≈Ø: {filename} ({len(df)} ≈ô√°dk≈Ø)...")
    chunks = []

    for index, row in df.iterrows():
        row_dict = row.to_dict()

        # Inteligentn√≠ hled√°n√≠ n√°zvu a k√≥du pro titulek
        nazev = "Nezn√°m√Ω p≈ôedmƒõt"
        kod = ""

        for k, v in row_dict.items():
            k_lower = str(k).lower()
            if "n√°zev" in k_lower or "nazev" in k_lower or "p≈ôedmƒõt" in k_lower:
                nazev = str(v)
            if "k√≥d" in k_lower or "zkratka" in k_lower or "code" in k_lower:
                kod = str(v)

        # Sestaven√≠ titulku
        if kod:
            title = f"P≈ôedmƒõt: {nazev} ({kod})"
        else:
            title = f"P≈ôedmƒõt: {nazev}"

        title = title.strip()

        # Sestaven√≠ obsahu (vyp√≠≈°eme v≈°echny sloupce)
        content_lines = [f"--- Detail z√°znamu: {title} ---"]
        for col_name, val in row_dict.items():
            if val and str(val).strip():  # Vynech√°me pr√°zdn√© bu≈àky
                content_lines.append(f"{col_name}: {val}")

        content = "\n".join(content_lines)

        chunks.append({
            "title": title,
            "content": content
        })

    return chunks


# --- 3. Embedding ---
def get_embedding(text):
    if not text or not text.strip():
        return None

    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    data = {"input": text, "model": EMBEDDING_MODEL}

    try:
        response = requests.post(OPENAI_EMBEDDING_URL, headers=headers, json=data)
        if response.status_code == 200:
            return np.array(response.json()["data"][0]["embedding"])
        else:
            print(f"‚ö†Ô∏è Chyba Embeddings API: {response.text}")
    except Exception as e:
        print(f"‚ö†Ô∏è Chyba p≈ôi embeddingu: {e}")

    return None


# --- HLAVN√ç LOOP ---
if __name__ == "__main__":
    init_db()

    print("üöÄ Startuji indexaci Google Disku...")
    service = get_drive_service()

    if service:
        files_data = get_files_recursive(service, GOOGLE_DRIVE_FOLDER_ID)

        print(f"‚úÖ Nalezeno a sta≈æeno celkem {len(files_data)} soubor≈Ø.")

        if files_data:
            # Sma≈æeme star√° data
            clear_database()
            print("üßπ Datab√°ze vyƒçi≈°tƒõna.")

            for i, file_item in enumerate(files_data):
                chunks = []

                # Vƒõtven√≠ logiky podle typu souboru
                if file_item.get("type") == "csv":
                    # CSV -> ≈ò√°dkov√Ω chunking
                    chunks = csv_row_chunking(file_item['data'], file_item['filename'])
                else:
                    # Text/PDF -> AI chunking
                    print(f"[{i + 1}/{len(files_data)}] Zpracov√°v√°m: {file_item['filename']}")
                    chunks = semantic_chunking(file_item['text'], file_item['filename'])

                if not chunks:
                    continue

                for chunk in chunks:
                    title = chunk.get("title", file_item['filename'])
                    text_content = chunk.get("content", "")

                    if text_content:
                        # --- KL√çƒåOV√â: Obohacen√≠ kontextu ---
                        # Vektor se poƒç√≠t√° z textu, kter√Ω obsahuje i n√°zev souboru a t√©ma.
                        # T√≠m ≈ôe≈°√≠me probl√©m, ≈æe "term√≠n" v jednom souboru znamen√° nƒõco jin√©ho ne≈æ v druh√©m.
                        enriched_text_for_embedding = (
                            f"Zdrojov√Ω soubor: {file_item['filename']}\n"
                            f"T√©ma: {title}\n"
                            f"Obsah: {text_content}"
                        )

                        emb = get_embedding(enriched_text_for_embedding)

                        if emb is not None:
                            insert_embedding_to_db(title, text_content, emb, file_item['filename'])

                            # U CSV nevypisujeme log pro ka≈æd√Ω ≈ô√°dek (bylo by to moc dlouh√©)
                            if file_item.get("type") != "csv":
                                print(f"   üíæ Ulo≈æeno: {title[:40]}...")

                if file_item.get("type") == "csv":
                    print(f"   ‚úÖ Ulo≈æeno {len(chunks)} z√°znam≈Ø z CSV tabulky.")

            print("üéâ Hotovo! V≈°echna data jsou v datab√°zi.")
        else:
            print("‚ö†Ô∏è ≈Ω√°dn√© relevantn√≠ soubory (PDF/DOCX/CSV) nenalezeny.")